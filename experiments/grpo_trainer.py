"""Custom implementation of GRPO (Generalized Reweighted Policy Optimization).

This module provides a light-weight trainer that can be used to fine-tune
language models with GRPO style updates.  The implementation is inspired by
publicly available projects such as the original GRPO release and the
``trl`` library from Hugging Face, but rewritten from scratch to keep the code
compact and easy to reason about.

Example
-------
The trainer expects a policy model that behaves like
``transformers.AutoModelForCausalLM`` (or any PyTorch module returning
``logits``) and, optionally, a frozen reference model.  A typical training loop
looks like::

    config = GRPOConfig()
    trainer = GRPOTrainer(policy_model, reference_model, tokenizer, config)
    for batch in dataloader:
        trainer.step(batch)

The batch should contain ``input_ids`` (prompt + response), ``attention_mask``
and a ``response_mask`` (1 for tokens generated by the policy, 0 otherwise)
along with scalar ``rewards`` for each example.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Iterable, Mapping, Optional

import torch
import torch.nn.functional as F
from torch import nn


@dataclass
class GRPOConfig:
    """Configuration container for :class:`GRPOTrainer`.

    Parameters
    ----------
    beta:
        Coefficient for the KL penalty against the reference model.  When the
        reference model is ``None`` the term is ignored.
    gamma:
        Temperature used when transforming advantages into normalized weights.
        Larger values put more mass on the top-scoring samples.
    value_coef:
        Weight of the optional value function regression loss.
    entropy_coef:
        Weight of the policy entropy bonus.  Useful to keep exploration early
        in training.
    max_grad_norm:
        Gradient clipping threshold.  ``None`` disables clipping.
    reward_scale:
        Multiplicative factor applied to the raw reward signal.
    normalize_advantages:
        If ``True``, subtract the mean advantage before exponentiation.
    detach_kl:
        When set the KL term is detached from the graph.  This behaviour is
        sometimes used in practice to stabilise training.
    """

    beta: float = 0.1
    gamma: float = 1.0
    value_coef: float = 0.1
    entropy_coef: float = 0.0
    max_grad_norm: Optional[float] = 1.0
    reward_scale: float = 1.0
    normalize_advantages: bool = True
    detach_kl: bool = False


class GRPOTrainer:
    """Minimal GRPO trainer for language model fine-tuning.

    The trainer performs a single optimisation step every time :meth:`step` is
    invoked.  The implementation intentionally avoids high-level abstractions
    to keep the core of the algorithm transparent.  It closely follows the
    description in *Generalized Reweighting for Policy Optimization* while
    staying compatible with causal language models from the Transformers
    ecosystem.
    """

    def __init__(
        self,
        policy_model: nn.Module,
        tokenizer,
        optimizer: torch.optim.Optimizer,
        *,
        reference_model: Optional[nn.Module] = None,
        value_head: Optional[nn.Module] = None,
        config: Optional[GRPOConfig] = None,
        device: Optional[torch.device] = None,
        use_wandb: bool = False,
    ) -> None:
        self.policy_model = policy_model
        self.reference_model = reference_model
        self.value_head = value_head
        self.tokenizer = tokenizer
        self.optimizer = optimizer
        self.config = config or GRPOConfig()
        self.device = device or next(policy_model.parameters()).device
        self.use_wandb = use_wandb
        self.step_count = 0

        if self.reference_model is not None:
            self.reference_model.to(self.device)
            self.reference_model.eval()

        self.policy_model.to(self.device)
        if self.value_head is not None:
            self.value_head.to(self.device)

    def step(self, batch: Mapping[str, torch.Tensor]) -> Dict[str, float]:
        """Run a single GRPO update.

        Parameters
        ----------
        batch:
            Mapping containing ``input_ids``, ``attention_mask``,
            ``response_mask`` and ``rewards``.  All tensors are moved to the
            trainer device automatically.

        Returns
        -------
        Dict[str, float]
            A dictionary with logging statistics.
        """

        batch = {k: v.to(self.device) for k, v in batch.items()}
        rewards = batch["rewards"] * self.config.reward_scale

        policy_out = self.policy_model(
            input_ids=batch["input_ids"],
            attention_mask=batch.get("attention_mask"),
            use_cache=False,
        )
        policy_log_probs = self._gather_log_probs(policy_out.logits, batch)

        ref_log_probs = torch.zeros_like(policy_log_probs)
        if self.reference_model is not None:
            with torch.no_grad():
                ref_out = self.reference_model(
                    input_ids=batch["input_ids"],
                    attention_mask=batch.get("attention_mask"),
                    use_cache=False,
                )
                ref_log_probs = self._gather_log_probs(ref_out.logits, batch)

        kl = policy_log_probs - ref_log_probs
        if self.config.detach_kl:
            kl_detached = kl.detach()
        else:
            kl_detached = kl

        advantages = rewards - self.config.beta * kl_detached
        if self.config.normalize_advantages:
            advantages = advantages - advantages.mean()

        weights = self._normalized_weights(advantages)
        policy_loss = -(weights * policy_log_probs).sum()

        stats = {
            "policy_loss": policy_loss.item(),
            "reward_mean": rewards.mean().item(),
            "reward_std": rewards.std().item(),
            "reward_min": rewards.min().item(),
            "reward_max": rewards.max().item(),
            "kl_mean": kl.mean().item(),
            "kl_std": kl.std().item(),
            "advantages_mean": advantages.mean().item(),
            "advantages_std": advantages.std().item(),
        }

        loss = policy_loss

        if self.value_head is not None:
            values = self.value_head(policy_out.last_hidden_state).squeeze(-1)
            value_targets = rewards.detach()
            value_loss = 0.5 * F.mse_loss(values, value_targets)
            loss = loss + self.config.value_coef * value_loss
            stats["value_loss"] = value_loss.item()

        if self.config.entropy_coef:
            entropy = self._sequence_entropy(policy_out.logits, batch)
            entropy_loss = -self.config.entropy_coef * entropy
            loss = loss + entropy_loss
            stats["entropy"] = entropy.item()

        self.optimizer.zero_grad()
        loss.backward()
        
        # Compute gradient norm before clipping
        total_norm = 0.0
        for p in self.policy_model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        if self.value_head is not None:
            for p in self.value_head.parameters():
                if p.grad is not None:
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
        total_norm = total_norm ** (1. / 2)
        stats["grad_norm"] = total_norm
        
        if self.config.max_grad_norm is not None:
            torch.nn.utils.clip_grad_norm_(
                list(self.policy_model.parameters())
                + ([] if self.value_head is None else list(self.value_head.parameters())),
                self.config.max_grad_norm,
            )
        self.optimizer.step()

        # Log to wandb if enabled
        if self.use_wandb:
            try:
                import wandb
                stats_with_step = {"step": self.step_count, **stats}
                wandb.log(stats_with_step)
            except ImportError:
                pass  # wandb not available, skip logging

        self.step_count += 1
        return stats

    def _gather_log_probs(
        self, logits: torch.Tensor, batch: Mapping[str, torch.Tensor]
    ) -> torch.Tensor:
        """Return sequence log probabilities for the response tokens.

        ``response_mask`` selects the part of the sequence corresponding to the
        sampled answer.  The resulting log-probabilities are summed across the
        response positions yielding a scalar per example.
        """

        input_ids = batch["input_ids"]
        response_mask = batch["response_mask"]

        log_probs = F.log_softmax(logits, dim=-1)
        token_log_probs = log_probs.gather(-1, input_ids.unsqueeze(-1)).squeeze(-1)
        seq_log_probs = (token_log_probs * response_mask).sum(dim=-1)
        return seq_log_probs

    def _sequence_entropy(
        self, logits: torch.Tensor, batch: Mapping[str, torch.Tensor]
    ) -> torch.Tensor:
        """Compute the average entropy over the sampled response tokens."""

        response_mask = batch["response_mask"]
        log_probs = F.log_softmax(logits, dim=-1)
        probs = log_probs.exp()
        token_entropy = -(probs * log_probs).sum(dim=-1)
        entropy = (token_entropy * response_mask).sum(dim=-1) / (
            response_mask.sum(dim=-1).clamp_min(1)
        )
        return entropy.mean()

    def _normalized_weights(self, advantages: torch.Tensor) -> torch.Tensor:
        """Turn raw advantages into a probability simplex."""

        scaled = advantages / max(self.config.gamma, 1e-8)
        weights = torch.softmax(scaled, dim=0)
        return weights.detach()


def collate_fn(samples: Iterable[Mapping[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
    """Batch policy rollouts into padded tensors.

    The function assumes every sample already contains the required keys and
    pads ``input_ids``/``attention_mask``/``response_mask`` to the maximum
    sequence length in the batch.  Rewards are stacked without modification.
    """

    materialized_samples = list(samples)
    if not materialized_samples:
        raise ValueError("collate_fn received an empty batch")

    keys = ["input_ids", "attention_mask", "response_mask"]
    max_length = max(sample["input_ids"].size(0) for sample in materialized_samples)
    batch: Dict[str, torch.Tensor] = {}
    for key in keys:
        tensors = [sample[key] for sample in materialized_samples]
        padded = []
        for tensor in tensors:
            pad_length = max_length - tensor.size(0)
            if pad_length > 0:
                padding = tensor.new_zeros((pad_length,))
                tensor = torch.cat([tensor, padding], dim=0)
            padded.append(tensor)
        batch[key] = torch.stack(padded, dim=0)

    batch["rewards"] = torch.stack(
        [sample["rewards"] for sample in materialized_samples]
    )
    return batch


__all__ = ["GRPOConfig", "GRPOTrainer", "collate_fn"]

